{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af40117a",
   "metadata": {},
   "source": [
    "# Introduction to PySpark\n",
    "\n",
    "PySpark is the Python API for Apache Spark, a powerful distributed computing system designed for big data processing and analytics. This tutorial will guide you through the fundamentals of PySpark, from basic concepts to practical applications.\n",
    "\n",
    "## What You'll Learn\n",
    "1. Setting up PySpark\n",
    "2. Creating and Working with SparkSession\n",
    "3. Basic DataFrame Operations\n",
    "4. Data Transformations and Actions\n",
    "5. SQL Operations in PySpark\n",
    "6. Data Cleaning and Preprocessing\n",
    "7. Advanced Operations and Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926481b7",
   "metadata": {},
   "source": [
    "## 1. Setting Up PySpark\n",
    "\n",
    "First, we'll import the necessary libraries and create our SparkSession, which is the entry point for PySpark functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('PySpark Tutorial') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print('SparkSession created successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d117f9e5",
   "metadata": {},
   "source": [
    "## 2. Creating Your First DataFrame\n",
    "\n",
    "Let's start by creating a simple DataFrame and exploring basic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363026ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple DataFrame\n",
    "data = [\n",
    "    (1, \"John\", 25),\n",
    "    (2, \"Alice\", 30),\n",
    "    (3, \"Bob\", 35)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"DataFrame Schema:\")\n",
    "df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf842eb",
   "metadata": {},
   "source": [
    "## 3. Basic DataFrame Operations\n",
    "\n",
    "Let's explore some common DataFrame operations that you'll use frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef04fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select specific columns\n",
    "df.select(\"name\", \"age\").show()\n",
    "\n",
    "# Filter data\n",
    "df.filter(df.age > 28).show()\n",
    "\n",
    "# Add a new column\n",
    "df_with_category = df.withColumn(\n",
    "    \"age_category\",\n",
    "    when(df.age < 30, \"Young\")\n",
    "    .when(df.age < 40, \"Middle\")\n",
    "    .otherwise(\"Senior\")\n",
    ")\n",
    "\n",
    "df_with_category.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85592e24",
   "metadata": {},
   "source": [
    "## 4. Working with Real Data\n",
    "\n",
    "Let's load and analyze some real-world data using the World Happiness dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8c7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file\n",
    "happiness_df = spark.read.csv('world_happiness_data/2023.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Schema:\")\n",
    "happiness_df.printSchema()\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "happiness_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dca5a3",
   "metadata": {},
   "source": [
    "## 5. Data Analysis and Aggregations\n",
    "\n",
    "Let's perform some common data analysis operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "happiness_df.describe().show()\n",
    "\n",
    "# Group by operations\n",
    "happiness_df.groupBy(\"Region\") \\\n",
    "    .agg(\n",
    "        avg(\"Happiness Score\").alias(\"avg_happiness\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    ) \\\n",
    "    .orderBy(\"avg_happiness\", ascending=False) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b41d03",
   "metadata": {},
   "source": [
    "## 6. Using SQL with PySpark\n",
    "\n",
    "PySpark also allows you to use SQL queries on DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dab392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "happiness_df.createOrReplaceTempView(\"happiness\")\n",
    "\n",
    "# Run SQL query\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        Region,\n",
    "        AVG(`Happiness Score`) as avg_happiness,\n",
    "        COUNT(*) as country_count\n",
    "    FROM happiness\n",
    "    GROUP BY Region\n",
    "    ORDER BY avg_happiness DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d20937c",
   "metadata": {},
   "source": [
    "## 7. Data Cleaning and Preprocessing\n",
    "\n",
    "Let's learn some common data cleaning operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b661397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "cleaned_df = happiness_df.dropna()\n",
    "\n",
    "# Fill missing values\n",
    "filled_df = happiness_df.fillna({\n",
    "    'Happiness Score': happiness_df.select(avg('Happiness Score')).collect()[0][0]\n",
    "})\n",
    "\n",
    "# Remove duplicates\n",
    "unique_df = happiness_df.dropDuplicates()\n",
    "\n",
    "# Print counts\n",
    "print(f\"Original count: {happiness_df.count()}\")\n",
    "print(f\"After removing nulls: {cleaned_df.count()}\")\n",
    "print(f\"After removing duplicates: {unique_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02016df",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Tips\n",
    "\n",
    "Here are some important best practices when working with PySpark:\n",
    "\n",
    "1. Always clean up your SparkSession when done\n",
    "2. Use caching wisely for frequently accessed DataFrames\n",
    "3. Monitor your transformations using explain()\n",
    "4. Optimize your queries for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274e4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of explaining the execution plan\n",
    "df_cached = happiness_df.cache()\n",
    "\n",
    "# Show the execution plan\n",
    "complex_query = df_cached.groupBy(\"Region\") \\\n",
    "    .agg(avg(\"Happiness Score\").alias(\"avg_happiness\")) \\\n",
    "    .filter(col(\"avg_happiness\") > 5.0)\n",
    "\n",
    "print(\"Execution Plan:\")\n",
    "complex_query.explain()\n",
    "\n",
    "# Clean up\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
